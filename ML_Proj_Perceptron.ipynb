{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fde5c45a",
   "metadata": {},
   "source": [
    "# Ioannis Gkionis\n",
    "\n",
    "## Multiple Layer Perceptron Model\n",
    "\n",
    "This was made as a part of a university project. The preprocessing is all done in functions.py and the MLP model is defined in perceptron.py. The code is all original and well-documented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86008ae9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9071, 784)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from main import *\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80d87a6",
   "metadata": {},
   "source": [
    "We run the default MLP model (learning rate:0,1 , max epochs:1000 , hidden layer neurons:64, tolerance=10^(-4)).\n",
    "\n",
    "The number of epochs that the model stops training at changes every time we run this because of the way weights are initialized (using np.random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1eac58c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Neural Network Classifier Accuracy:  0.9686486486486486\n",
      "Model:  Neural Network Classifier  stopped training at epoch number  266\n"
     ]
    }
   ],
   "source": [
    "run_MLP()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb0500d",
   "metadata": {},
   "source": [
    "We then get the 4 values from 10^(-2) to 0.5 for the learning rate and from 2 to 2^10 for the hidden layer neurons. I avoided using the lower values for the learning rate because it always lead to underflows, however the code should still run for lower values, just change the first argument on the lr_values declaration from -2 to -5. There should still be some underflows and overflows for extreme values however they do not seem to affect the end results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e06c7c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kharnifex\\Desktop\\MLP\\functions.py:25: RuntimeWarning: overflow encountered in exp\n",
      "  return 1.0 / (1.0 + np.exp(-z))\n"
     ]
    }
   ],
   "source": [
    "values_both=4\n",
    "lr_values = np.logspace(-2, math.log(0.5, 10), num=values_both, base=10).tolist()\n",
    "hl_values = np.logspace(1, 10, num=values_both, base=2).tolist()\n",
    "hl_values = [int(a) for a in hl_values]#logspace returns float\n",
    "\n",
    "final_loss_MLP = []\n",
    "\n",
    "for lr in lr_values:\n",
    "    for hl in hl_values:\n",
    "        nn = NeuralNetwork(hid=hl, epochs=500, learning_rate=lr, tol=1e-4)\n",
    "        g = nn.train(X_train, y_train, X_valid, y_valid)\n",
    "        final_loss_MLP.append(nn.losses[-1])#store last epoch's validation cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b948fe",
   "metadata": {},
   "source": [
    "We can then find the best model by searching for the one with the smallest validation cost value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34ceb348",
   "metadata": {},
   "outputs": [],
   "source": [
    "o = [index for index, item in enumerate(final_loss_MLP) if item == min(final_loss_MLP)]\n",
    "x = o[0] % values_both\n",
    "y = o[0] // values_both"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b8b6fc",
   "metadata": {},
   "source": [
    "We can then look at all the values of the loss function for all possible models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9edaa1b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6925406576611719,\n",
       " 0.6922371621660284,\n",
       " 0.5566445521269311,\n",
       " 0.3138068170649446,\n",
       " 0.512587856023977,\n",
       " 0.23021763285443725,\n",
       " 0.11113223186181755,\n",
       " 0.09301125510465355,\n",
       " 0.3065026939675531,\n",
       " 0.105819105337835,\n",
       " 0.062248924240297654,\n",
       " 0.057195604154115336,\n",
       " 0.18705489740570705,\n",
       " 0.07574861696626556,\n",
       " 0.05068030797857148,\n",
       " 0.05210781458475902]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_loss_MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d0d4c7",
   "metadata": {},
   "source": [
    "if we compare the loss of models with the same amount of hidden layer neurons but different learning rates, let's say the 2nd, 6th, 10th and 14th ones, we can clearly see that the validation loss decreases as the learning rate increases. however, we know that if we make it so that the range of values for the learning rate is bigger, the larger values will lead to overfitting where the models will have really good training accuracy/cost but will struggle a lot with the validation and testing sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109ba2fc",
   "metadata": {},
   "source": [
    "Since the weight values aren't stored anywhere (wouldn't be a good idea to store multiple models on my poor 16GB of RAM), we will train another model with the same parameters as the one we deemed to be better performing. note that error values etc. will slightly change because of the randomly initiated weight arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "624af1e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Neural Network Classifier Accuracy:  0.9762162162162162\n",
      "Model stopped training at epoch number  385\n"
     ]
    }
   ],
   "source": [
    "nn = NeuralNetwork(hid=hl_values[x], epochs=500, learning_rate=lr_values[y], tol=1e-4)\n",
    "g = nn.train(X_train, y_train, X_valid, y_valid)\n",
    "nn.show_predictions(X_test, y_test)\n",
    "print('Model stopped training at epoch number ', g)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
